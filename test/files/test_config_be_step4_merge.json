{
  "name": "inventory triggers test",
  "description": "Test cases for BE workflow step 4: merged rows",
  "author": "Deepak",
  "date": "20180711",
  "sql_path": "/Users/deepak/bi2/rfk-business-insights/etl/sql/email_marketing/behavioral_email/tests",
  "script_path": "/Users/deepak/rfk-data-etl/em/behavioral_email/workflow_step4_merge_queue_rows_hourly.sql",
  "before_once": [
    "USE TEST",
    "SET DATE='20180723'",
    "SET HOUR='03'",
    "INSERT INTO EM.last_etl_exec_date_hour ( etl_date, etl_hour ) VALUES ('20180723','02')"
  ],
  "before_each": [
    "TRUNCATE TABLE TEST.EM.be_workflow_step4_merged_rows",
    "TRUNCATE TABLE TEST.EM.be_workflow_queue",
    "TRUNCATE TABLE TEST.EM.be_workflow_step3_mapped_rows",
    "TRUNCATE TABLE TEST.EM.test_be_workflow_step4_merged_rows"
  ],
  "tests": [
    {
      "name": "test1",
      "description": "data in queue table and no data in current batch",
      "before_test": [
        "snowsql>put file:///Users/deepak/Downloads/input_queue.json @TEST.TEMP.mystage",
        "snowsql>copy into TEST.EM.be_workflow_queue from (select parse_json($1):CKEY, parse_json($1):ACCOUNT, parse_json($1):DOMAIN_HASH, parse_json($1):DOMAIN_NAME,parse_json($1):ETL_DATE,parse_json($1):ETL_HOUR,parse_json($1):ULID,parse_json($1):ULID_TYPE,parse_json($1):UUID,parse_json($1):TRIGGER_GROUP,parse_json($1):TRIGGER_NAME,parse_json($1):TRIGGER_TIMESTAMP,parse_json($1):PAYLOAD,parse_json($1):REASONS from @TEST.TEMP.mystage/input_queue.json.gz t)"
      ],
      "asserts": [
        {
          "sql": "SELECT count(*) FROM TEST.EM.be_workflow_step4_merged_rows",
          "expected": [
            "snowsql>put file:///Users/deepak/Downloads/exp_step4_merged.json @TEST.TEMP.mystage",
            "snowsql>copy into TEST.EM.test_be_workflow_step4_merged_rows from (select parse_json($1):CKEY, parse_json($1):ACCOUNT, parse_json($1):DOMAIN_HASH, parse_json($1):DOMAIN_NAME,parse_json($1):ETL_DATE,parse_json($1):ETL_HOUR,parse_json($1):ULID,parse_json($1):ULID_TYPE,parse_json($1):UUID,parse_json($1):TRIGGER_GROUP,parse_json($1):TRIGGER_NAME,parse_json($1):TRIGGER_TIMESTAMP,parse_json($1):SOURCE,parse_json($1):PAYLOAD from @TEST.TEMP.mystage/exp_step4_merged.json.gz t)"
          ],
          "message": "count of expected and actual rows should be same"
        },
        {
          "sql": "SELECT distinct SOURCE FROM TEST.EM.be_workflow_step4_merged_rows",
          "expected": [],
          "message": "The source column value should be SOURCE "
        }
      ],
      "after_test": []
    },
    {
      "name": "test2",
      "description": "data in both queue table and current_batch",
      "before_test": [
        "snowsql>copy into TEST.EM.be_workflow_queue from (select parse_json($1):CKEY, parse_json($1):ACCOUNT, parse_json($1):DOMAIN_HASH, parse_json($1):DOMAIN_NAME,parse_json($1):ETL_DATE,parse_json($1):ETL_HOUR,parse_json($1):ULID,parse_json($1):ULID_TYPE,parse_json($1):UUID,parse_json($1):TRIGGER_GROUP,parse_json($1):TRIGGER_NAME,parse_json($1):TRIGGER_TIMESTAMP,parse_json($1):PAYLOAD,parse_json($1):REASONS from @TEST.TEMP.mystage/input_queue.json.gz t)",
        "snowsql>put file:///Users/deepak/Downloads/input_step4_merged.json @TEST.TEMP.mystage",
        "snowsql>copy into TEST.EM.be_workflow_step3_mapped_rows from (select parse_json($1):CKEY, parse_json($1):ACCOUNT, parse_json($1):DOMAIN_HASH, parse_json($1):DOMAIN_NAME,parse_json($1):ETL_DATE,parse_json($1):ETL_HOUR,parse_json($1):ULID,parse_json($1):ULID_TYPE,parse_json($1):UUID,parse_json($1):TRIGGER_GROUP,parse_json($1):TRIGGER_NAME,parse_json($1):TRIGGER_TIMESTAMP,parse_json($1):PAYLOAD from @TEST.TEMP.mystage/input_step4_merged.json.gz t)"
      ],
      "asserts": [
        {
          "sql": "SELECT count(*) FROM TEST.EM.be_workflow_step4_merged_rows",
          "expected": [
            "snowsql>put file:///Users/deepak/Downloads/exp_step4_1_merged.json @TEST.TEMP.mystage",
            "snowsql>copy into TEST.EM.test_be_workflow_step4_merged_rows from (select parse_json($1):CKEY, parse_json($1):ACCOUNT, parse_json($1):DOMAIN_HASH, parse_json($1):DOMAIN_NAME,parse_json($1):ETL_DATE,parse_json($1):ETL_HOUR,parse_json($1):ULID,parse_json($1):ULID_TYPE,parse_json($1):UUID,parse_json($1):TRIGGER_GROUP,parse_json($1):TRIGGER_NAME,parse_json($1):TRIGGER_TIMESTAMP,parse_json($1):SOURCE,parse_json($1):PAYLOAD from @TEST.TEMP.mystage/exp_step4_1_merged.json.gz t)"
          ],
          "message": "count of expected and actual rows should be same"
        },
        {
          "sql": "SELECT distinct SOURCE FROM TEST.EM.be_workflow_step4_merged_rows",
          "expected": [],
          "message": "The source column value should be SOURCE and CURRENT_BATCH "
        },
        {
          "sql":"SELECT * FROM TEST.EM.be_workflow_step4_merged_rows",
          "expected":[],
          "message":"both table data should be equal"
        }
      ],
      "after_test": []
    }
  ],
  "after_each": [],
  "after_once": [
    "UNSET DATE",
    "UNSET HOUR"
  ]
}